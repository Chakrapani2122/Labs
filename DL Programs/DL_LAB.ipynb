{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp4Ii0fOhecQ"
      },
      "source": [
        "# Deep Learning Lab Programs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VImJC3rNhecS"
      },
      "source": [
        "1. Setting up the Spyder IDE Environment and Executing a Python Program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_m1cMMbhecT"
      },
      "source": [
        "Python is a versatile programming language that has gained popularity due to its ease of use, wide range of libraries and frameworks, and active community of developers. Having an Integrated Development Environment (IDE) can be beneficial when working with Python, especially for larger projects. An IDE provides an environment to write, test, and debug code, making it easier to manage your projects. Spyder is one such IDE designed specifically for scientific computing with Python.\n",
        "\n",
        "## Installing Spyder IDE\n",
        "Spyder IDE can be installed on Windows, macOS, and Linux operating systems. There are several ways to install the software.\n",
        "Installing Spyder using Anaconda\n",
        "\n",
        "One of the easiest ways to install Spyder IDE is to use Anaconda, a popular Python distribution with many pre-installed packages, including Spyder IDE. The following steps outline how to install Spyder IDE using Anaconda:\n",
        "\n",
        "* Download the Anaconda distribution for your operating system fromtheofficial website: https://www.anaconda.com/products/individual\n",
        "* Install Anaconda by following the installation wizard.\n",
        "* Once installed, open the Anaconda Navigator application.\n",
        "* Click on the \"Environments\" tab in the left panel and select the \"base (root)\" environment.\n",
        "* In the \"base (root)\" environment, search for \"spyder\" in the search bar.\n",
        "* Click on the \"Install\" button next to \"spyder\" to install the latest version of Spyder IDE.\n",
        "\n",
        "\n",
        "## Installing Spyder using pip\n",
        "Another way to install Spyder IDE is to use pip, a package manager for Python. The following steps outline how to install Spyder IDE using pip:\n",
        "\n",
        "* Open the terminal/command prompt on your system.\n",
        "* Type the following command to install Spyder using pip:\n",
        "\n",
        "\t\t`pip install Spyder`  \n",
        "\n",
        "* Once installed, you can open Spyder IDE by typing the following command in the terminal/command prompt:\n",
        "\n",
        "\t\t`spyder`\n",
        "\n",
        "## Installing Spyder using the official website\n",
        "You can also download and install Spyder IDE from the official website: The following steps outline how to install Spyder IDE using the official website:\n",
        "\n",
        "* Go to the official Spyder IDE website: https://www.spyder-ide.org/.\n",
        "* Choose \"Download\" from the top menu's button selection.\n",
        "* Choose the right download link for your operating system.\n",
        "* Follow the installation wizard to install Spyder IDE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGa6Ls-UhecU"
      },
      "source": [
        "### Python Program to Make a Simple Calculator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giLlDHDMhecU",
        "outputId": "451f37cd-6159-457a-81a5-65a43db939aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please select the operation.\n",
            "a. Add\n",
            "b. Subtract\n",
            "c. Multiply\n",
            "d. Divide\n",
            "20  +  10  =  30\n"
          ]
        }
      ],
      "source": [
        "def add(P, Q):\n",
        "   return P + Q\n",
        "def subtract(P, Q):\n",
        "   return P - Q\n",
        "def multiply(P, Q):\n",
        "   return P * Q\n",
        "def divide(P, Q):\n",
        "   return P / Q\n",
        "\n",
        "print (\"Please select the operation.\")\n",
        "print (\"a. Add\")\n",
        "print (\"b. Subtract\")\n",
        "print (\"c. Multiply\")\n",
        "print (\"d. Divide\")\n",
        "\n",
        "choice = input(\"Please enter choice (a/ b/ c/ d): \")\n",
        "\n",
        "num_1 = int (input (\"Please enter the first number: \"))\n",
        "num_2 = int (input (\"Please enter the second number: \"))\n",
        "\n",
        "if choice == 'a':\n",
        "   print (num_1, \" + \", num_2, \" = \", add(num_1, num_2))\n",
        "\n",
        "elif choice == 'b':\n",
        "   print (num_1, \" - \", num_2, \" = \", subtract(num_1, num_2))\n",
        "\n",
        "elif choice == 'c':\n",
        "   print (num1, \" * \", num2, \" = \", multiply(num1, num2))\n",
        "\n",
        "elif choice == 'd':\n",
        "   print (num_1, \" / \", num_2, \" = \", divide(num_1, num_2))\n",
        "\n",
        "else:\n",
        "   print (\"This is an invalid input\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPx72U4LhecW"
      },
      "source": [
        "### Python Function to Display Calendar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ0HPeVKhecX",
        "outputId": "e32d0b06-8ef7-4f0b-d711-fc533c051179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    August 2000\n",
            "Mo Tu We Th Fr Sa Su\n",
            "    1  2  3  4  5  6\n",
            " 7  8  9 10 11 12 13\n",
            "14 15 16 17 18 19 20\n",
            "21 22 23 24 25 26 27\n",
            "28 29 30 31\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import calendar\n",
        "yy = int(input(\"Enter year: \"))\n",
        "mm = int(input(\"Enter month: \"))\n",
        "print(calendar.month(yy,mm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvCmjijOhecX"
      },
      "source": [
        "2. Installing Keras, Tensorflow and Pytorch libraries and making use of them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_0wZHmhecY"
      },
      "source": [
        "Install Python: Ensure you have Python installed on your system. . You can download Python from the official website(https://www.python.org/downloads/) and follow the installation instructions.\n",
        "\n",
        "Create a virtual environment (optional but recommended): It's good practice to create a virtual environment to keep your project dependencies isolated. You can use virtualenv or conda to create .Ex virtualenv :\n",
        "Install TensorFlow, Keras, and PyTorch: Now that you have your virtual environment set up (if you chose to use one), you can install the libraries using pip:\n",
        "\n",
        "3.\tInstall TensorFlow, Keras, and PyTorch: Now that you have your virtual environment set up (if you chose to use one), you can install the libraries using pip:\n",
        "\n",
        "#### What is Deep Learning?\n",
        "\n",
        "Deep learning and machine learning are part of the artificial intelligence family, though deep learning is also a subset of machine learning. Understanding these concepts is essential for any discussion of Keras vs TensorFlow vs PyTorch.\n",
        "\n",
        "Deep learning imitates the human brain’s neural pathways in processing data, using it for decision-making, detecting objects, recognizing speech, and translating languages. It learns without human supervision or intervention, pulling from unstructured and unlabeled data.\n",
        "\n",
        "#### What is Keras?\n",
        "Keras is an effective high-level neural network Application Programming Interface (API) written in Python. This open-source neural network library is designed to provide fast experimentation with deep neural networks, and it can run on top of CNTK, TensorFlow, and Theano.\n",
        "Keras focuses on being modular, user-friendly, and extensible. It doesn’t handle low-level computations; instead, it hands them off to another library called the Backend.\n",
        "\n",
        "Keras was adopted and integrated into TensorFlow in mid-2017. Users can access it via the tf.keras module. However, the Keras library can still operate separately and independently.\n",
        "\n",
        "\n",
        "#### What is PyTorch?\n",
        "PyTorch is a relatively new deep learning framework based on Torch. Developed by Facebook’s AI research group and open-sourced on GitHub in 2017, it’s used for natural language processing applications. PyTorch has a reputation for simplicity, ease of use, flexibility, efficient memory usage, and dynamic computational graphs. It also feels native, making coding more manageable and increasing processing speed.\n",
        "\n",
        "#### What is TensorFlow?\n",
        "TensorFlow is an end-to-end open-source deep learning framework developed by Google and released in 2015. It is known for documentation and training support, scalable production and deployment options, multiple abstraction levels, and support for different platforms, such as Android.\n",
        "\n",
        "TensorFlow is a symbolic math library used for neural networks and is best suited for dataflow programming across a range of tasks. It offers multiple abstraction levels for building and training models.\n",
        "\n",
        "A promising and fast-growing entry in the world of deep learning, TensorFlow offers a flexible, comprehensive ecosystem of community resources, libraries, and tools that facilitate building and deploying machine learning apps.\n",
        "\n",
        "|  | Keras  | PyTorch | TensorFlow |\n",
        "| :---: | :---: | :---: | :---: |\n",
        "| API Level | High | Low | High and Low |\n",
        "| Architecture | Simple, concise, readable | Large datasets, high performance | Large datasets, high performance |\n",
        "| Datasets | Smaller datasets | Complex, less readable | Not easy to use |\n",
        "| Debugging | Simple network, so debugging is not often needed | Good debugging capabilities | Difficult to conduct debugging |\n",
        "| Does It Have Trained Models? | Yes | Yes | Yes |\n",
        "| Popularity | Most popular | Third most popular | Second most popular |\n",
        "| Speed | Slow, low performance | Fast, high-performance | Fast, high-performance |\n",
        "| Written In | Python | Lua | C++, CUDA, Python |\n",
        "\n",
        "\n",
        "#### Which is Better PyTorch or TensorFlow or Keras?\n",
        "Everyone’s situation and needs are different, so we use them based on the most for your AI project.\n",
        "\n",
        "Let's start by installing the libraries:\n",
        "\n",
        "1.TensorFlow: TensorFlow is a popular deep learning library developed by Google.\n",
        "   You can install it using pip:\n",
        "\n",
        "    `pip install tensorflow`\n",
        "2. Keras: Keras is a high-level API built on top of TensorFlow, designed to make deep learning easier and more accessible. Since TensorFlow 2.0, Keras is integrated into TensorFlow, so there's no need to install it separately.\n",
        "\n",
        "It can be imported directly from TensorFlow:\n",
        "\n",
        "`import tensorflow as tf`\n",
        "\n",
        "`from tensorflow import keras`\n",
        "\n",
        "3. PyTorch: PyTorch is another popular deep learning library developed by Facebook.    \n",
        "You can install it using pip:\n",
        "\n",
        "\t`pip install torch`\n",
        "\n",
        "\n",
        "Now, let's provide some basic usage examples:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIWvQ_w0hecZ"
      },
      "source": [
        "#### TensorFlow/Keras Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg3lF1v7hecZ",
        "outputId": "be89bdce-f430-4fae-9a25-509051f32bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 1s 5ms/step - loss: 0.7410 - accuracy: 0.4300\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.7404 - accuracy: 0.4300\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7393 - accuracy: 0.4300\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7387 - accuracy: 0.4300\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7383 - accuracy: 0.4400\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.7387 - accuracy: 0.4500\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7370 - accuracy: 0.4600\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7367 - accuracy: 0.4600\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7362 - accuracy: 0.4600\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7359 - accuracy: 0.4500\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "Predictions:\n",
            "[[0.512555  ]\n",
            " [0.4276121 ]\n",
            " [0.5426533 ]\n",
            " [0.636594  ]\n",
            " [0.6112259 ]\n",
            " [0.5614948 ]\n",
            " [0.49505144]\n",
            " [0.5892617 ]\n",
            " [0.6244462 ]\n",
            " [0.63725394]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "data = np.random.random((100, 20))\n",
        "labels = np.random.randint(2, size=(100, 1))\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a dense layer with one neuron (binary classification)\n",
        "model.add(Dense(1, input_dim=20, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(data, labels, epochs=10, batch_size=10)\n",
        "\n",
        "# Predict using the trained model\n",
        "new_data = np.random.random((10, 20))\n",
        "predictions = model.predict(new_data)\n",
        "\n",
        "print(\"Predictions:\")\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv0C3HFCheca"
      },
      "source": [
        "#### PyTorch Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM8TPn4jheca",
        "outputId": "6921972f-1cbc-4bc6-dd36-21e5260b94fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.7295\n",
            "Epoch [2/10], Loss: 0.7280\n",
            "Epoch [3/10], Loss: 0.7265\n",
            "Epoch [4/10], Loss: 0.7251\n",
            "Epoch [5/10], Loss: 0.7237\n",
            "Epoch [6/10], Loss: 0.7223\n",
            "Epoch [7/10], Loss: 0.7210\n",
            "Epoch [8/10], Loss: 0.7196\n",
            "Epoch [9/10], Loss: 0.7184\n",
            "Epoch [10/10], Loss: 0.7171\n",
            "Predictions:\n",
            "tensor([[0.5480],\n",
            "        [0.5814],\n",
            "        [0.5871],\n",
            "        [0.6004],\n",
            "        [0.6337],\n",
            "        [0.5235],\n",
            "        [0.6132],\n",
            "        [0.6406],\n",
            "        [0.5371],\n",
            "        [0.5750]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Create some toy data\n",
        "data = torch.tensor(np.random.random((100, 20)), dtype=torch.float32)\n",
        "labels = torch.tensor(np.random.randint(2, size=(100, 1)), dtype=torch.float32)\n",
        "\n",
        "# Define a simple neural network model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(20, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Predict using the trained model\n",
        "new_data = torch.tensor(np.random.random((10, 20)), dtype=torch.float32)\n",
        "predictions = model(new_data)\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obbkqlRmSjWh"
      },
      "source": [
        "3. Applying the Convolution Neural Network on computer vision problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVQzuecoRqgq",
        "outputId": "d2179506-9388-49a1-b043-f29ca778842b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 29s 37ms/step - loss: 0.2114 - accuracy: 0.9356 - val_loss: 0.0766 - val_accuracy: 0.9752\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 28s 37ms/step - loss: 0.0533 - accuracy: 0.9824 - val_loss: 0.0462 - val_accuracy: 0.9866\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 30s 40ms/step - loss: 0.0378 - accuracy: 0.9879 - val_loss: 0.0413 - val_accuracy: 0.9871\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 28s 38ms/step - loss: 0.0294 - accuracy: 0.9904 - val_loss: 0.0405 - val_accuracy: 0.9889\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 28s 37ms/step - loss: 0.0242 - accuracy: 0.9921 - val_loss: 0.0422 - val_accuracy: 0.9868\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0316 - accuracy: 0.9898\n",
            "Test accuracy: 0.989799976348877\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_labels, test_labels = to_categorical(train_labels), to_categorical(test_labels)\n",
        "\n",
        "# Build and compile CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(28, 28, 1)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94L9fXw3SmJR"
      },
      "source": [
        "4. Image classification on MNIST dataset (CNN model with Fully connected layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jhly67USUAP",
        "outputId": "cb6e55d6-fb3d-4dc5-8177-76a66b3c1279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 31s 41ms/step - loss: 0.2267 - accuracy: 0.9286 - val_loss: 0.0789 - val_accuracy: 0.9769\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 28s 37ms/step - loss: 0.0623 - accuracy: 0.9804 - val_loss: 0.0623 - val_accuracy: 0.9817\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 29s 38ms/step - loss: 0.0433 - accuracy: 0.9865 - val_loss: 0.0496 - val_accuracy: 0.9852\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 28s 38ms/step - loss: 0.0335 - accuracy: 0.9890 - val_loss: 0.0470 - val_accuracy: 0.9871\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 28s 37ms/step - loss: 0.0261 - accuracy: 0.9914 - val_loss: 0.0469 - val_accuracy: 0.9870\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0349 - accuracy: 0.9882\n",
            "Test accuracy: 0.9882000088691711\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_labels, test_labels = to_categorical(train_labels), to_categorical(test_labels)\n",
        "\n",
        "# Build and compile CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Viva\n",
        "\n",
        "1. **What is the purpose of using Convolutional Neural Networks (CNNs) in image classification?**\n",
        "   - **Answer:** CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images, making them suitable for image classification tasks.\n",
        "\n",
        "2. **Why is normalization applied to the MNIST images by dividing by 255.0?**\n",
        "   - **Answer:** Normalization scales pixel values to the range [0, 1], which helps in faster convergence during training and makes optimization more stable.\n",
        "\n",
        "3. **How does the MaxPooling layer contribute to the CNN model?**\n",
        "   - **Answer:** MaxPooling reduces the spatial dimensions of the feature maps and retains the most prominent features, improving computational efficiency and increasing the model's ability to generalize.\n",
        "\n",
        "4. **Why are ReLU (Rectified Linear Unit) activation functions used in the Conv2D layers?**\n",
        "   - **Answer:** ReLU introduces non-linearity and helps the network learn complex patterns in the data by allowing it to model more complex functions.\n",
        "\n",
        "5. **What is the role of the Flatten layer in the CNN model?**\n",
        "   - **Answer:** The Flatten layer reshapes the 2D feature maps into a 1D vector, which is required for connecting to fully connected layers.\n",
        "\n",
        "6. **Why is the number 64 used for the first Dense layer in the model architecture?**\n",
        "   - **Answer:** The number 64 represents the number of neurons in the fully connected layer, which acts as a feature extractor before the final output layer.\n",
        "\n",
        "7. **How does the 'categorical_crossentropy' loss function work in this image classification task?**\n",
        "   - **Answer:** Categorical cross-entropy measures the dissimilarity between the predicted class probabilities and the true class labels, encouraging the model to make more accurate predictions.\n",
        "\n",
        "8. **What is the purpose of using the 'adam' optimizer in this model?**\n",
        "   - **Answer:** The 'adam' optimizer is an efficient gradient descent optimization algorithm that adapts learning rates for each parameter, making it suitable for training deep neural networks.\n",
        "\n",
        "9. **Why is there a validation split of 0.2 during model training?**\n",
        "   - **Answer:** A validation split allows monitoring the model's performance on a separate dataset, helping to prevent overfitting and making sure the model generalizes well.\n",
        "\n",
        "10. **How is the test accuracy calculated, and why is it an important metric for evaluating the model's performance?**\n",
        "    - **Answer:** Test accuracy is calculated by evaluating the trained model on a separate test dataset. It measures how well the model performs on unseen data, indicating its real-world effectiveness in image classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om7Y-rhxWIy_",
        "outputId": "ff01ff92-b4a7-4d98-9280-0ad721a50e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.4952 - accuracy: 0.7430 - val_loss: 0.3077 - val_accuracy: 0.8728\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 38s 121ms/step - loss: 0.2373 - accuracy: 0.9083 - val_loss: 0.2659 - val_accuracy: 0.8892\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 38s 123ms/step - loss: 0.0889 - accuracy: 0.9737 - val_loss: 0.2988 - val_accuracy: 0.8838\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.0256 - accuracy: 0.9948 - val_loss: 0.3688 - val_accuracy: 0.8864\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.0069 - accuracy: 0.9995 - val_loss: 0.4231 - val_accuracy: 0.8856\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.4389 - accuracy: 0.8825\n",
            "Test accuracy: 0.8824800252914429\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Load and preprocess IMDb dataset\n",
        "num_words = 10000\n",
        "max_len = 200\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "train_data, test_data = pad_sequences(train_data, maxlen=max_len), pad_sequences(test_data, maxlen=max_len)\n",
        "\n",
        "# Build and compile CNN model for sentiment analysis\n",
        "model = Sequential([\n",
        "    Embedding(num_words, 128, input_length=max_len),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.fit(train_data, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Viva\n",
        "\n",
        "1. **What is the purpose of the 'imdb' dataset in this program?**\n",
        "   - **Answer:** The 'imdb' dataset is used for sentiment analysis, containing movie reviews labeled as positive or negative to train and evaluate a model's ability to classify sentiment.\n",
        "\n",
        "2. **Why do we pad sequences with 'pad_sequences' in NLP tasks like this one?**\n",
        "   - **Answer:** Padding sequences ensures that all input texts have the same length, which is required for processing in neural networks, allowing efficient batch processing.\n",
        "\n",
        "3. **How does the 'Embedding' layer contribute to the model's architecture?**\n",
        "   - **Answer:** The 'Embedding' layer is responsible for converting integer word indices into dense vectors, which helps the model understand the semantic meaning of words in the input text.\n",
        "\n",
        "4. **Why is 'Conv1D' used in this model?**\n",
        "   - **Answer:** 'Conv1D' (1D convolution) captures local patterns in the text, allowing the model to identify important features and relationships in the input data.\n",
        "\n",
        "5. **What is the significance of 'GlobalMaxPooling1D' in this program?**\n",
        "   - **Answer:** 'GlobalMaxPooling1D' reduces the dimensionality of the features while preserving the most important information, making it a global feature extraction step.\n",
        "\n",
        "6. **How does the 'Dropout' layer contribute to the model's training and generalization?**\n",
        "   - **Answer:** 'Dropout' helps prevent overfitting by randomly deactivating a fraction of neurons during training, forcing the model to be more robust and generalize better.\n",
        "\n",
        "7. **Why is the 'sigmoid' activation function used in the final layer of this model?**\n",
        "   - **Answer:** The 'sigmoid' activation function is appropriate for binary classification tasks, where it maps the model's output to probabilities of a positive sentiment (1) or a negative sentiment (0).\n",
        "\n",
        "8. **What is the role of the 'binary_crossentropy' loss function in this sentiment analysis model?**\n",
        "   - **Answer:** 'binary_crossentropy' measures the dissimilarity between the predicted sentiment scores and the true sentiment labels, helping the model learn to make accurate sentiment predictions.\n",
        "\n",
        "9. **Why is there a 'validation_split' of 0.2 during model training?**\n",
        "   - **Answer:** The 'validation_split' parameter sets aside a portion of the training data for validation, helping to monitor the model's performance and prevent overfitting during training.\n",
        "\n",
        "10. **How is test accuracy calculated, and why is it a crucial metric in NLP sentiment analysis?**\n",
        "    - **Answer:** Test accuracy is determined by evaluating the model on a separate test dataset. It measures the model's ability to correctly classify sentiment in unseen texts, indicating its real-world performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki-uFHjsW0a1"
      },
      "source": [
        "6. Train a sentiment analysis model on IMDB dataset, use RNN layers with LSTM/GRU notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-FpDcflWM5K",
        "outputId": "572db89a-1a95-40c9-c7b3-8d0a0cf4284a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 167s 527ms/step - loss: 0.4500 - accuracy: 0.7800 - val_loss: 0.3994 - val_accuracy: 0.8232\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 161s 513ms/step - loss: 0.2653 - accuracy: 0.9000 - val_loss: 0.3206 - val_accuracy: 0.8682\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 156s 498ms/step - loss: 0.1877 - accuracy: 0.9303 - val_loss: 0.3868 - val_accuracy: 0.8422\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 161s 513ms/step - loss: 0.1390 - accuracy: 0.9514 - val_loss: 0.4129 - val_accuracy: 0.8596\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 158s 503ms/step - loss: 0.1046 - accuracy: 0.9621 - val_loss: 0.4197 - val_accuracy: 0.8424\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.4235 - accuracy: 0.8432\n",
            "Test accuracy: 0.8432400226593018\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Load and preprocess the IMDB dataset\n",
        "num_words = 10000  # Consider the top 10,000 most frequent words\n",
        "max_len = 200      # Limit each review to 200 words\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "train_data = pad_sequences(train_data, maxlen=max_len)\n",
        "test_data = pad_sequences(test_data, maxlen=max_len)\n",
        "\n",
        "# Build the LSTM-based sentiment analysis model\n",
        "model = Sequential([\n",
        "    Embedding(num_words, 128, input_length=max_len),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Viva\n",
        "\n",
        "1. **How does the 'num_words' parameter impact data preprocessing?**\n",
        "   - **Answer:** 'num_words' specifies the maximum number of frequent words to consider, allowing you to control the vocabulary size and filter out less common words from the dataset.\n",
        "\n",
        "2. **What is the significance of padding sequences with 'pad_sequences'?**\n",
        "   - **Answer:** Padding sequences ensures that all input text reviews have the same length, which is essential for creating consistent input data for the RNN model.\n",
        "\n",
        "3. **Why is an 'Embedding' layer used in the model architecture?**\n",
        "   - **Answer:** The 'Embedding' layer transforms word indices into dense vector representations, capturing the semantic meaning of words and improving the model's understanding of the text.\n",
        "\n",
        "4. **How does the 'LSTM' layer differ from standard RNNs in handling long-range dependencies?**\n",
        "   - **Answer:** LSTMs are designed to better capture long-range dependencies in the text by using a gating mechanism that allows them to remember and forget information selectively.\n",
        "\n",
        "5. **Explain the role of 'dropout' and 'recurrent_dropout' in the LSTM layer.**\n",
        "   - **Answer:** 'dropout' drops out connections within the LSTM cell, preventing overfitting. 'recurrent_dropout' does the same for recurrent connections, improving the model's generalization.\n",
        "\n",
        "6. **Why is the 'sigmoid' activation function used in the final layer of this model?**\n",
        "   - **Answer:** 'sigmoid' activation is suitable for binary classification tasks, mapping model output to probabilities of positive sentiment (1) or negative sentiment (0).\n",
        "\n",
        "7. **What does the 'binary_crossentropy' loss function measure in this sentiment analysis model?**\n",
        "   - **Answer:** 'binary_crossentropy' quantifies the difference between predicted sentiment scores and the actual sentiment labels, aiding the model in learning accurate sentiment predictions.\n",
        "\n",
        "8. **How does the 'adam' optimizer help in training the model?**\n",
        "   - **Answer:** 'adam' is an efficient optimization algorithm that adapts learning rates for each parameter, making it suitable for training deep neural networks like the RNN in this model.\n",
        "\n",
        "9. **Why is there a 'validation_split' of 0.2 during model training?**\n",
        "   - **Answer:** The 'validation_split' parameter reserves a portion of the training data for validation, helping to monitor the model's performance and prevent overfitting during training.\n",
        "\n",
        "10. **Explain the significance of 'Test accuracy' in evaluating the model's performance.**\n",
        "    - **Answer:** 'Test accuracy' measures how well the model can correctly classify sentiment in unseen text reviews, demonstrating its real-world effectiveness in sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89TY210BW7Bu"
      },
      "source": [
        "7. Applying the Autoencoder algorithms for encoding the real-world data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "HCGlQ2GSW87Q",
        "outputId": "d5d361f0-0597-4a53-b464-d41e83b68a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.3632 - val_loss: 0.3113\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.3017 - val_loss: 0.2987\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.2935 - val_loss: 0.2926\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.2887 - val_loss: 0.2888\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.2856 - val_loss: 0.2862\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.2832 - val_loss: 0.2845\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.2812 - val_loss: 0.2832\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.2797 - val_loss: 0.2811\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.2785 - val_loss: 0.2800\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.2774 - val_loss: 0.2791\n",
            "313/313 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x400 with 20 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess Fashion MNIST dataset\n",
        "(train_images, _), (test_images, _) = fashion_mnist.load_data()\n",
        "train_images, test_images = train_images.astype('float32') / 255.0, test_images.astype('float32') / 255.0\n",
        "train_images, test_images = train_images.reshape((-1, 28 * 28)), test_images.reshape((-1, 28 * 28))\n",
        "\n",
        "# Build autoencoder model\n",
        "model = Sequential([\n",
        "    Input(shape=(28 * 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(28 * 28, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_images, epochs=10, batch_size=256, shuffle=True, validation_data=(test_images, test_images))\n",
        "\n",
        "# Encode and decode test images\n",
        "decoded_images = model.predict(test_images)\n",
        "\n",
        "# Display original and decoded images\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(test_images[i].reshape(28, 28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_images[i].reshape(28, 28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Viva\n",
        "\n",
        "1. **What is the purpose of the 'fashion_mnist' dataset in this program?**\n",
        "   - **Answer:** The 'fashion_mnist' dataset is used for training and evaluating an Autoencoder model, which aims to learn a compact representation of fashion item images.\n",
        "\n",
        "2. **Why is data normalization applied by dividing pixel values by 255.0?**\n",
        "   - **Answer:** Normalization scales pixel values to the range [0, 1], which is a common practice to aid in model training and ensure consistent input data.\n",
        "\n",
        "3. **How is the Autoencoder model architecture structured?**\n",
        "   - **Answer:** The model consists of an encoding part with two dense layers and a decoding part with two dense layers, with the central layer acting as a bottleneck.\n",
        "\n",
        "4. **What is the purpose of the 'activation' functions used in the model?**\n",
        "   - **Answer:** 'relu' (Rectified Linear Unit) activations in the encoding layers and 'sigmoid' activation in the decoding layer introduce non-linearity and control information flow.\n",
        "\n",
        "5. **Why is 'binary_crossentropy' used as the loss function in this Autoencoder model?**\n",
        "   - **Answer:** 'binary_crossentropy' measures the dissimilarity between the input and the output, which is suitable for reconstructing binary data like images.\n",
        "\n",
        "6. **How does the model 'encode' the input images into a lower-dimensional representation?**\n",
        "   - **Answer:** The model learns to represent the input images with reduced dimensions by training on the encoding layers.\n",
        "\n",
        "7. **Explain the significance of 'shuffling' the data during training.**\n",
        "   - **Answer:** Shuffling ensures that the model sees the data in a random order during each epoch, which can prevent the model from learning the order of the data and overfitting.\n",
        "\n",
        "8. **What does the 'decoded_images' variable represent in the program?**\n",
        "   - **Answer:** 'decoded_images' contains the reconstructed images produced by the Autoencoder, showing how well the model can recreate the input data.\n",
        "\n",
        "9. **Why is it important to visualize original and decoded images?**\n",
        "   - **Answer:** Visualization helps assess the quality of the Autoencoder's encoding and decoding process and visually confirm the model's performance.\n",
        "\n",
        "10. **How does the Autoencoder serve as a dimensionality reduction technique?**\n",
        "    - **Answer:** By encoding and decoding data, the Autoencoder learns to represent it in a lower-dimensional space, making it useful for dimensionality reduction and feature extraction tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE0KTux1Xv2z"
      },
      "source": [
        "8. Applying Generative Adversial Networks for image generation and unsupervised tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ukNUPTB5Xxw-",
        "outputId": "0acd0ec8-4145-4a3b-e9ed-4a5549dd43db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 0, D Loss: 1.1584377884864807, G Loss: 0.7845238447189331\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 100, D Loss: 0.35241855680942535, G Loss: 0.8277177810668945\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 200, D Loss: 0.43394961953163147, G Loss: 0.8306236863136292\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 300, D Loss: 0.46397434920072556, G Loss: 0.8422479033470154\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 400, D Loss: 0.4918534606695175, G Loss: 0.9002684354782104\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "Epoch 500, D Loss: 0.6007169187068939, G Loss: 0.8704537749290466\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 600, D Loss: 0.6388996094465256, G Loss: 0.8466131687164307\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 700, D Loss: 0.6437913477420807, G Loss: 0.8727812767028809\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 800, D Loss: 0.6343975067138672, G Loss: 0.8752001523971558\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 900, D Loss: 0.6332091987133026, G Loss: 0.9443280696868896\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def build_generator(latent_dim):\n",
        "    return Sequential([\n",
        "        Dense(128, input_dim=latent_dim),\n",
        "        LeakyReLU(0.2),\n",
        "        Dense(784, activation='tanh'),\n",
        "        Reshape((28, 28))\n",
        "    ])\n",
        "\n",
        "def build_discriminator(img_shape):\n",
        "    return Sequential([\n",
        "        Flatten(input_shape=img_shape),\n",
        "        Dense(128),\n",
        "        LeakyReLU(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "img_shape = (28, 28)\n",
        "discriminator = build_discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "latent_dim = 100\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "discriminator.trainable = False\n",
        "z = Input(shape=(latent_dim,))\n",
        "gen_img = generator(z)\n",
        "validity = discriminator(gen_img)\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "(train_images, _), (_, _) = mnist.load_data()\n",
        "train_images = (train_images.astype(np.float32) - 127.5) / 127.5\n",
        "train_images = np.expand_dims(train_images, axis=-1)\n",
        "\n",
        "epochs, batch_size, sample_interval = 1000, 128, 100\n",
        "for epoch in range(epochs):\n",
        "    idx = np.random.randint(0, train_images.shape[0], batch_size)\n",
        "    real_imgs = train_images[idx]\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((batch_size, 1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "    if epoch % sample_interval == 0:\n",
        "        print(f\"Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "\n",
        "        if epoch % (sample_interval * 10) == 0:\n",
        "            gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "            fig, axs = plt.subplots(1, 5)\n",
        "            for i in range(5):\n",
        "                axs[i].imshow(gen_imgs[i], cmap='gray')\n",
        "                axs[i].axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    # Add a break condition to exit the loop after a certain number of epochs\n",
        "    if epoch == epochs - 1:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Viva\n",
        "1. **How does a GAN work at a high level?**\n",
        "   - **Answer:** A GAN consists of two neural networks, a generator and a discriminator, that work in opposition. The generator creates fake data, and the discriminator tries to distinguish real from fake data.\n",
        "\n",
        "2. **What is the role of the 'build_generator' function in the program?**\n",
        "   - **Answer:** 'build_generator' constructs the generator model, which creates synthetic images based on random noise.\n",
        "\n",
        "3. **Why is the 'LeakyReLU' activation function used in the generator and discriminator networks?**\n",
        "   - **Answer:** LeakyReLU helps prevent vanishing gradients and allows the networks to capture complex patterns in the data.\n",
        "\n",
        "4. **What is the purpose of 'discriminator.trainable = False'?**\n",
        "   - **Answer:** This line freezes the discriminator during training of the combined GAN model, allowing the generator to be trained without interference from the discriminator.\n",
        "\n",
        "5. **How is the 'binary_crossentropy' loss function used for the discriminator and the combined model?**\n",
        "   - **Answer:** The 'binary_crossentropy' loss measures how well the discriminator distinguishes between real and fake data, and it's used to update the weights of both the discriminator and the generator.\n",
        "\n",
        "6. **Why are the real images normalized to the range [-1, 1] using (x/127.5 - 1) in the program?**\n",
        "   - **Answer:** This normalization helps improve GAN stability and ensures that the generator's output also falls within the same range for compatibility.\n",
        "\n",
        "7. **Explain the purpose of the 'sample_interval' variable in the program.**\n",
        "   - **Answer:** 'sample_interval' determines how often the program displays generated images and tracks the training progress.\n",
        "\n",
        "8. **What does 'd_loss_real' and 'd_loss_fake' represent in the training loop?**\n",
        "   - **Answer:** 'd_loss_real' is the discriminator's loss when distinguishing real images, and 'd_loss_fake' is the loss when distinguishing fake images.\n",
        "\n",
        "9. **How does the generator 'learn' from the discriminator's feedback?**\n",
        "   - **Answer:** The generator aims to minimize the discriminator's ability to differentiate between real and fake data, so it adjusts its parameters based on the feedback provided by the discriminator's loss.\n",
        "\n",
        "10. **Why is there a break condition in the loop that exits after a certain number of epochs?**\n",
        "    - **Answer:** A break condition is added to ensure that the training process stops after a predefined number of epochs, preventing the program from running indefinitely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
